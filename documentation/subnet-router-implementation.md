# Subnet Router Implementation - Headscale Mesh VPN

**Status**: Planning / Investigation Complete
**Last Updated**: 2025-10-04
**Related**: Headscale deployed on Management VM (10.10.20.10:8082)

---

## Executive Summary

### Goal
Enable VPN clients to access the Trusted LAN (10.10.10.0/24) via Headscale mesh VPN by deploying a Subnet Router that advertises routes to VPN peers.

### Architecture
```
VPN Client (anywhere)
  ↓ WireGuard tunnel (Headscale control plane)
  ↓ 100.64.x.x mesh network
Subnet Router VM (Alpine Linux)
  ↓ Routes traffic to LAN
  ↓ 10.10.10.0/24
Trusted LAN devices
```

### Key Components
- **Headscale**: Already deployed at 10.10.20.10:8082 (VPN control server)
- **Subnet Router**: Alpine Linux VM (101) with dual interfaces
  - eth0: 10.10.10.10 (LAN VLAN 10)
  - eth1: 10.10.20.11 (Services VLAN 20)
- **Tailscale Client**: Installed on Subnet Router, connects to Headscale
- **Pre-Auth Keys**: Generated by Headscale for automated registration

---

## Investigation Findings

### Finding 1: Alpine Cloud-Init Package Directive Support

**Research Date**: 2025-10-04

**Current cloud-init usage**:
```yaml
package_update: true
package_upgrade: true

packages:
  - curl
  - ca-certificates
```

**Discovery**:
- ✅ `package_update` and `package_upgrade` directives **work correctly**
- ❌ `packages:` directive is **NOT reliably supported** on Alpine 3.19
- ⚠️ Alpine cloud-init documentation does not list Alpine as supported distro for package installation module

**Evidence**:
- Official Alpine README.Alpine mentions module limitations
- Community forums show users prefer `runcmd: apk add` over `packages:`
- Current subnet router VM does not have curl/ca-certificates installed despite being in packages list

**Recommendation**:
Use `runcmd:` with explicit `apk add` commands instead of `packages:` directive.

**Corrected Pattern**:
```yaml
package_update: true
package_upgrade: true

runcmd:
  - apk add curl ca-certificates tailscale iptables
  - echo 'net.ipv4.ip_forward=1' >> /etc/sysctl.conf
  - sysctl -p
```

### Finding 2: Tailscale Availability in Alpine

**Research Date**: 2025-10-04

**Package Availability**:
- ✅ Available in Alpine 3.19 **community repository**
- ✅ Community repository **already enabled** on Alpine cloud images
- ✅ Package name: `tailscale`
- ✅ Service manager: OpenRC (Alpine's init system)

**Installation Commands**:
```bash
apk add tailscale
rc-update add tailscale default  # Enable on boot
rc-service tailscale start       # Start now
```

**Verified Repository Configuration**:
```
/etc/apk/repositories contains:
https://dl-cdn.alpinelinux.org/alpine/v3.19/main
https://dl-cdn.alpinelinux.org/alpine/v3.19/community  ← Tailscale here
```

**No additional repository configuration needed.**

### Finding 3: Subnet Router Network Connectivity Issues

**Discovered**: 2025-10-04

**Network Configuration (from cloud-init)**:
```
eth0: 10.10.10.10/24, gateway 10.10.10.1 (OPNsense LAN interface)
eth1: 10.10.20.11/24, no gateway
```

**Connectivity Test Results**:
- ✅ Can ping Services VLAN (10.10.20.10) - 0% packet loss
- ❌ Cannot ping LAN gateway (10.10.10.1) - 100% packet loss
- ❌ Cannot reach internet (1.1.1.1) - 100% packet loss
- ❌ Cannot download Alpine packages from CDN

**Root Cause**:
Default gateway is set to `10.10.10.1` (OPNsense LAN interface on VLAN 10), but:
- OPNsense firewall rules may not be routing VLAN 10 traffic yet
- Alternative gateway `10.10.20.1` (OPNsense Services VLAN) is reachable and routes to internet

**Impact**:
Cannot install packages via `apk add` during cloud-init if internet access required.

**Solution Options**:

**Option A**: Fix OPNsense firewall rules for VLAN 10 routing
- Pro: Proper network architecture
- Con: Requires OPNsense investigation and reconfiguration

**Option B**: Use Services VLAN gateway during bootstrap
- Pro: Works immediately, Services VLAN has working internet
- Con: Requires changing gateway in cloud-init, then switching back
- Implementation:
  ```yaml
  write_files:
    - path: /etc/network/interfaces
      content: |
        auto eth0
        iface eth0 inet static
            address 10.10.10.10/24
            # No gateway on eth0 during bootstrap

        auto eth1
        iface eth1 inet static
            address 10.10.20.11/24
            gateway 10.10.20.1  # Use Services gateway for bootstrap

  runcmd:
    - apk add tailscale iptables
    - # ... configure tailscale ...
    - # Switch gateway back to LAN after setup
    - sed -i 's/gateway 10.10.20.1//' /etc/network/interfaces
    - echo '    gateway 10.10.10.1' >> /etc/network/interfaces.d/eth0
    - rc-service networking restart
  ```

**Recommended**: Option B for immediate implementation, Option A as follow-up task.

### Finding 4: Headscale Authentication Model

**Research Date**: 2025-10-04

**Pre-Authentication Keys**:
Headscale uses pre-auth keys for automated node registration (no manual approval needed).

**Key Properties**:
- **One-time or reusable**: `--reusable` flag allows multiple nodes
- **Time-limited**: `--expiration 24h` (default 1h)
- **User-scoped**: `--user 1` (admin user already exists)
- **Ephemeral option**: `--ephemeral` nodes auto-delete after inactivity (DON'T USE)

**Key Lifecycle**:
1. Generate: `headscale preauthkeys create --user 1 --reusable --expiration 24h`
2. Use: Pass to `tailscale up --authkey=<key>`
3. Node registers immediately
4. Key can expire, **node stays registered forever**

**Critical Insight**: Pre-auth key is ONLY for initial registration, not for staying connected.

**Node Persistence**:
- ✅ Node registration persists in Headscale SQLite database
- ✅ WireGuard keys persist in `/var/lib/tailscale/`
- ✅ Both survive reboots on both sides
- ✅ Tunnels auto-reconnect after reboot
- ✅ **Zero intervention needed after initial setup**

### Finding 5: Route Advertisement and Approval

**Subnet Routes in Headscale**:

**Advertise Routes** (on Subnet Router):
```bash
tailscale up \
  --login-server=http://10.10.20.10:8082 \
  --authkey=<preauth-key> \
  --advertise-routes=10.10.10.0/24 \
  --accept-dns=false
```

**Approve Routes** (on Headscale server):
```bash
# List advertised routes
podman exec headscale headscale routes list

# Approve route
podman exec headscale headscale routes enable <route-id>
```

**Route Persistence**:
- ✅ Routes approved in Headscale persist in database
- ✅ Survives Headscale container restart
- ✅ Survives Subnet Router reboot
- ✅ **One-time approval, works forever**

**Client Routing**:
Once routes approved:
- VPN clients automatically receive routing table
- Traffic to 10.10.10.0/24 routes through Subnet Router node
- No client-side configuration needed

### Finding 6: IP Forwarding Requirement

**Kernel Configuration**:
Subnet Router must forward packets between VPN (tailscale0) and LAN (eth0).

**Enable IP Forwarding**:
```bash
# Runtime
sysctl -w net.ipv4.ip_forward=1

# Persist across reboots
echo 'net.ipv4.ip_forward=1' >> /etc/sysctl.conf
```

**Verification**:
```bash
cat /proc/sys/net/ipv4/ip_forward  # Should output: 1
```

**Cloud-Init Pattern**:
```yaml
write_files:
  - path: /etc/sysctl.d/ip_forward.conf
    permissions: '0644'
    content: |
      net.ipv4.ip_forward=1

runcmd:
  - sysctl -p /etc/sysctl.d/ip_forward.conf
```

---

## Implementation Plan

### Decision: Simplified Single-Playbook Approach

**Rejected**: Separate configuration playbook (`subnet-router-configure.yml`)
**Chosen**: Embed configuration into deployment playbook via cloud-init

**Rationale**:
- ✅ VM boots fully configured on first boot
- ✅ Single playbook execution
- ✅ Pre-auth key generated just-in-time, never stored
- ✅ Follows existing pattern (all setup in cloud-init)
- ✅ Reduces orchestration complexity

### Modified Playbook Flow

**File**: `ansible/playbooks/infrastructure/subnet-router-deploy.yml`

#### New Task: Generate Pre-Auth Key (before VM creation)

```yaml
- name: Generate Headscale pre-auth key for subnet router
  delegate_to: localhost
  shell: |
    ssh debian@10.10.20.10 \
      'sudo podman exec headscale headscale preauthkeys create \
        --user 1 \
        --reusable \
        --expiration 24h \
        -o json' | \
      jq -r '.key'
  register: headscale_preauth
  changed_when: false

- name: Set pre-auth key fact
  set_fact:
    preauth_key: "{{ headscale_preauth.stdout }}"
```

**Note**: Executed on Proxmox host, SSHs to Management VM to run headscale command.

#### Modified Cloud-Init Configuration

```yaml
- name: Generate cloud-init user-data
  copy:
    dest: "/var/lib/vz/snippets/subnet-router-{{ vmid }}.yml"
    mode: '0644'
    content: |
      #cloud-config
      hostname: {{ vm_name }}
      manage_etc_hosts: true

      users:
        - name: alpine
          sudo: ALL=(ALL) NOPASSWD:ALL
          shell: /bin/sh
          lock_passwd: false
          plain_text_passwd: alpine

      ssh_pwauth: true

      package_update: true
      package_upgrade: true

      write_files:
        - path: /etc/network/interfaces
          permissions: '0644'
          content: |
            auto lo
            iface lo inet loopback

            auto eth0
            iface eth0 inet static
                address {{ lan_ip }}/{{ lan_netmask }}
                # Gateway added after Tailscale setup

            auto eth1
            iface eth1 inet static
                address {{ services_ip }}/{{ services_netmask }}
                gateway {{ services_gateway }}  # Use for bootstrap internet access

        - path: /etc/sysctl.d/ip_forward.conf
          permissions: '0644'
          content: |
            net.ipv4.ip_forward=1

      runcmd:
        # Enable IP forwarding
        - sysctl -p /etc/sysctl.d/ip_forward.conf

        # Install packages (using Services VLAN gateway for internet)
        - apk add tailscale iptables

        # Configure Tailscale service
        - rc-update add tailscale default
        - rc-service tailscale start
        - sleep 5

        # Connect to Headscale and advertise routes
        - tailscale up --login-server=http://10.10.20.10:8082 --authkey={{ preauth_key }} --advertise-routes=10.10.10.0/24 --accept-dns=false

        # Switch gateway to LAN for production use
        - echo '    gateway {{ lan_gateway }}' >> /etc/network/interfaces
        - rc-service networking restart

        - echo "Subnet router configured"

      final_message: "Subnet router VM deployed after $UPTIME seconds"
```

**Key Changes**:
1. Use Services VLAN gateway (10.10.20.1) during bootstrap for internet access
2. Install packages via `runcmd:` instead of `packages:` directive
3. Connect to Headscale with pre-auth key during first boot
4. Switch to LAN gateway after setup complete

#### New Task: Approve Advertised Routes (after VM starts)

```yaml
- name: Wait for Tailscale node to register in Headscale
  delegate_to: localhost
  shell: |
    ssh debian@10.10.20.10 \
      'sudo podman exec headscale headscale nodes list -o json' | \
      jq -r '.[] | select(.name | contains("subnet-router")) | .id'
  register: node_check
  until: node_check.stdout != ""
  retries: 12
  delay: 5
  changed_when: false

- name: Get advertised route ID for LAN subnet
  delegate_to: localhost
  shell: |
    ssh debian@10.10.20.10 \
      'sudo podman exec headscale headscale routes list -o json' | \
      jq -r '.routes[] | select(.prefix == "10.10.10.0/24") | .id'
  register: route_id
  changed_when: false

- name: Approve LAN subnet route
  delegate_to: localhost
  shell: |
    ssh debian@10.10.20.10 \
      'sudo podman exec headscale headscale routes enable {{ route_id.stdout }}'
  when: route_id.stdout != ""
  changed_when: true
```

**Flow**:
1. Wait for node to appear in Headscale (up to 60 seconds)
2. Find route ID for 10.10.10.0/24
3. Approve route via Headscale CLI

#### Updated Deployment Summary

```yaml
- name: Display deployment summary
  debug:
    msg:
      - ""
      - "=========================================="
      - "   SUBNET ROUTER DEPLOYMENT COMPLETE"
      - "=========================================="
      - ""
      - "VM Details:"
      - "  VM ID: {{ vmid }}"
      - "  VM Name: {{ vm_name }}"
      - "  LAN IP: {{ lan_ip }}"
      - "  Services IP: {{ services_ip }}"
      - ""
      - "Headscale Integration:"
      - "  Node registered: {{ node_check.stdout }}"
      - "  Route advertised: 10.10.10.0/24"
      - "  Route approved: {{ route_id.stdout }}"
      - ""
      - "VPN Access:"
      - "  VPN clients can now access Trusted LAN via this router"
      - "  Subnet: 10.10.10.0/24"
      - ""
      - "Next Steps:"
      - "  1. Create VPN client: headscale preauthkeys create --user 1"
      - "  2. Connect client: tailscale up --login-server=http://10.10.20.10:8082"
      - "  3. Access LAN: ping 10.10.10.10 (this router)"
      - ""
```

### Variables

**New Variables** (added to playbook vars):
```yaml
# Services VLAN configuration
services_gateway: "10.10.20.1"

# Headscale configuration
headscale_url: "http://10.10.20.10:8082"
```

### Idempotency

**Check if already configured**:
```yaml
- name: Check if subnet router already registered in Headscale
  delegate_to: localhost
  shell: |
    ssh debian@10.10.20.10 \
      'sudo podman exec headscale headscale nodes list -o json' | \
      jq -r '.[] | select(.name | contains("subnet-router")) | .id'
  register: existing_node
  failed_when: false
  changed_when: false

- name: Skip deployment if already configured
  meta: end_play
  when: existing_node.stdout != ""
```

**Alternative**: Allow re-running, skip pre-auth key generation if node exists.

---

## Semaphore Integration

### Pre-Auth Key Storage Strategy

**Decision**: DO NOT store pre-auth keys in Semaphore

**Rationale**:
- Pre-auth keys are ephemeral (expire in hours/days)
- Only needed during initial registration
- Storing adds no value (nodes stay registered after key expires)
- Reduces secret sprawl

**Flow**:
```
Playbook starts
  ↓
Generate pre-auth key (runtime)
  ↓
Pass to cloud-init as variable
  ↓
VM boots, uses key to register
  ↓
Key expires (24h later)
  ↓
Node stays registered forever
```

### HeadscaleAPI Environment

**Already Exists**: Created during Headscale deployment

**Contents**:
```json
{
  "HEADSCALE_API_KEY": "<api-key>",
  "HEADSCALE_GRPC_URL": "10.10.20.10:50443"
}
```

**Usage**: For future API-based management (creating users, managing ACLs, etc.)

**Not needed for subnet router deployment** - uses CLI via podman exec instead.

### Template Configuration

**Playbook**: `ansible/playbooks/infrastructure/subnet-router-deploy.yml`

**Semaphore Template Metadata**:
```yaml
template_config:
  semaphore_environment: "SemaphoreAPI"  # For API access to create inventory/keys
  semaphore_inventory: "proxmox"          # Runs on Proxmox host
```

**Template Name**: "Subnet Router 1: Create Alpine VM"

**Already generated by template sync** - no changes needed.

---

## Persistent Configuration

### What Survives Reboots

**On Subnet Router VM**:
- ✅ IP forwarding (`/etc/sysctl.d/ip_forward.conf`)
- ✅ Tailscale service auto-start (`rc-update add tailscale default`)
- ✅ Tailscale state (`/var/lib/tailscale/tailscaled.state`)
- ✅ WireGuard keys (in Tailscale state)
- ✅ Network interfaces (`/etc/network/interfaces`)

**On Headscale Server**:
- ✅ Node registration (`/var/lib/headscale/db.sqlite`)
- ✅ Route approvals (in database)
- ✅ User accounts (in database)

**On VPN Clients**:
- ✅ Routing table (pushed by Headscale)
- ✅ Automatic reconnection to mesh

### What Requires Zero Intervention

After successful deployment:
- ✅ Subnet Router survives reboot (Tailscale auto-starts)
- ✅ Headscale survives reboot (container restart policy: always)
- ✅ Routes persist across reboots
- ✅ VPN clients automatically reconnect
- ✅ Traffic flows without manual steps

**Only manual action**: Initial deployment (run playbook once)

---

## Testing Strategy

### Pre-Deployment Tests

1. **Verify Headscale Running**
   ```bash
   curl http://10.10.20.10:8082/health
   # Expected: {"status":"pass"}
   ```

2. **Verify Headscale User Exists**
   ```bash
   ssh debian@10.10.20.10 'sudo podman exec headscale headscale users list'
   # Expected: admin user with ID 1
   ```

### Deployment Tests

1. **Run Playbook**
   ```bash
   cd /opt/privatebox
   ansible-playbook ansible/playbooks/infrastructure/subnet-router-deploy.yml
   ```

2. **Verify VM Creation**
   ```bash
   qm list | grep subnet-router
   # Expected: VM 101 running
   ```

3. **Verify Tailscale Registration**
   ```bash
   ssh debian@10.10.20.10 'sudo podman exec headscale headscale nodes list'
   # Expected: subnet-router node listed
   ```

4. **Verify Route Advertisement**
   ```bash
   ssh debian@10.10.20.10 'sudo podman exec headscale headscale routes list'
   # Expected: 10.10.10.0/24 advertised by subnet-router, enabled: true
   ```

### Post-Deployment Verification

**On Subnet Router**:
```bash
# SSH to subnet router
ssh alpine@10.10.20.11

# Check Tailscale status
tailscale status
# Expected: Connected, serving subnet routes

# Check IP forwarding
cat /proc/sys/net/ipv4/ip_forward
# Expected: 1

# Check routes
ip route show | grep tailscale
# Expected: tailscale0 interface routes
```

**On Headscale**:
```bash
ssh debian@10.10.20.10 'sudo podman exec headscale headscale nodes list'
# Expected: subnet-router online, connected

ssh debian@10.10.20.10 'sudo podman exec headscale headscale routes list'
# Expected: 10.10.10.0/24 enabled: true
```

### VPN Client Testing

**Create Test Client**:
```bash
# Generate pre-auth key for test client
ssh debian@10.10.20.10 'sudo podman exec headscale headscale preauthkeys create --user 1 -o json' | jq -r '.key'

# On test device (laptop/phone):
tailscale up --login-server=http://10.10.20.10:8082 --authkey=<key>
```

**Connectivity Tests**:
```bash
# From VPN client
ping 10.10.10.10          # Subnet router LAN IP
ping 10.10.20.10          # Management VM
ping 10.10.20.1           # OPNsense Services interface

# Web access
curl http://10.10.20.10:8080   # AdGuard
curl http://10.10.20.10:3000   # Semaphore
```

**Expected Results**:
- ✅ All pings succeed
- ✅ Can access web UIs
- ✅ Traffic routes through Subnet Router

---

## Troubleshooting

### Issue: Node won't register

**Symptoms**:
- `tailscale up` hangs or fails
- Node doesn't appear in `headscale nodes list`

**Checks**:
```bash
# On Subnet Router
rc-service tailscale status   # Should be running
tailscale status               # Check error messages
ping 10.10.20.10              # Can reach Headscale?
curl http://10.10.20.10:8082/health  # Headscale responding?
```

**Solutions**:
- Regenerate pre-auth key (may have expired)
- Check Headscale logs: `podman logs headscale`
- Verify clock sync (Tailscale sensitive to time skew)

### Issue: Routes not advertised

**Symptoms**:
- Node registered but no routes in `headscale routes list`

**Checks**:
```bash
# On Subnet Router
tailscale status
# Should show "offering routes: 10.10.10.0/24"

# Check tailscale args
ps aux | grep tailscale
# Should see --advertise-routes=10.10.10.0/24
```

**Solutions**:
- Re-run `tailscale up` with `--advertise-routes` flag
- Check IP forwarding enabled: `cat /proc/sys/net/ipv4/ip_forward`

### Issue: Routes advertised but not approved

**Symptoms**:
- Routes show in `headscale routes list` but `enabled: false`
- VPN clients can't access LAN

**Check**:
```bash
ssh debian@10.10.20.10 'sudo podman exec headscale headscale routes list'
# Look for enabled: false
```

**Solution**:
```bash
# Get route ID
ROUTE_ID=$(ssh debian@10.10.20.10 'sudo podman exec headscale headscale routes list -o json' | jq -r '.routes[] | select(.prefix == "10.10.10.0/24") | .id')

# Approve route
ssh debian@10.10.20.10 "sudo podman exec headscale headscale routes enable $ROUTE_ID"
```

### Issue: VPN client can't reach LAN

**Symptoms**:
- Client connected to Headscale
- Can't ping 10.10.10.x addresses

**Checks**:
```bash
# On VPN client
tailscale status
# Verify connected, check routes

ip route show
# Should see route to 10.10.10.0/24 via tailscale0

# On Subnet Router
ip a show tailscale0
# Should have 100.64.x.x address

ping <client-tailscale-ip>
# Test bidirectional connectivity
```

**Solutions**:
- Check routes approved in Headscale
- Verify IP forwarding on Subnet Router
- Check firewall rules on Subnet Router (iptables)
- Verify OPNsense allows traffic from 10.10.10.10

### Issue: Internet access fails after gateway switch

**Symptoms**:
- Subnet Router can access Services VLAN
- Cannot reach internet or LAN gateway

**Check**:
```bash
# On Subnet Router
ip route show
# Verify default gateway is 10.10.10.1

ping 10.10.10.1  # OPNsense LAN interface
```

**Solutions**:
- **Root cause**: OPNsense VLAN 10 routing not configured
- **Temporary fix**: Keep Services gateway (10.10.20.1)
- **Permanent fix**: Configure OPNsense firewall rules for VLAN 10
- See: `documentation/network-architecture/opnsense-configuration-requirements.md`

---

## Future Enhancements

### Multi-Subnet Support
Advertise multiple subnets:
```bash
tailscale up --advertise-routes=10.10.10.0/24,10.10.20.0/24,10.10.40.0/24
```

### Exit Node Configuration
Allow VPN clients to route all internet traffic through Subnet Router:
```bash
tailscale up --advertise-exit-node
```

### ACL Policies
Use Headscale ACLs to restrict which VPN clients can access which subnets.

### Monitoring
- Bandwidth usage tracking
- Connection uptime monitoring
- Route availability alerts

---

## References

### Documentation
- Headscale: https://headscale.net/stable/
- Tailscale Subnet Routers: https://tailscale.com/kb/1019/subnets
- Alpine Cloud-Init: https://git.alpinelinux.org/aports/tree/community/cloud-init/README.Alpine

### Related Files
- Subnet Router Deploy: `ansible/playbooks/infrastructure/subnet-router-deploy.yml`
- Headscale Deploy: `ansible/playbooks/services/headscale-deploy.yml`
- Network Architecture: `documentation/network-architecture/vlan-design.md`

### Community Resources
- Alpine Tailscale: https://techoverflow.net/2022/05/11/how-to-install-tailscale-in-alpine-linux/
- Headscale GitHub: https://github.com/juanfont/headscale

---

## Status

- [x] Research completed
- [x] Alpine cloud-init limitations documented
- [x] Tailscale availability verified
- [x] Network connectivity issues identified
- [x] Pre-auth key flow designed
- [x] Implementation plan documented
- [ ] Playbook modifications implemented
- [ ] Testing completed
- [ ] OPNsense VLAN 10 routing fixed

**Next Steps**:
1. Implement modified cloud-init configuration
2. Test deployment on clean system
3. Fix OPNsense VLAN 10 routing (separate task)
4. Create VPN client setup documentation
